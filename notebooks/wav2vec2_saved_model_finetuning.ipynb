{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('t5': conda)"
    },
    "interpreter": {
      "hash": "184c0bf4d405d4a36e719b504ff2a22c838d19108535bf816dff1a5aad495b87"
    },
    "colab": {
      "name": "wav2vec2-saved-model-finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rWk8nL6Ui-_0",
        "wvuJL8-f0zn5",
        "oPp18ZHRtnq-",
        "1mvTuOXpwsQe",
        "7Vlm3ySFULsG",
        "SJtSLgCvxvBG",
        "SJfPlTgezD0i"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ab42871d4ce4ec3926439d2db14c766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_672481d2d0164171bfb0fd0d08916bd7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_73ea04eded764748ad24c6c6e63a84e6",
              "IPY_MODEL_3db5fd5cdbf443c7b8ccf4da4260363c"
            ]
          }
        },
        "672481d2d0164171bfb0fd0d08916bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73ea04eded764748ad24c6c6e63a84e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_83c5f0808cf74dbeb1c54da91cf6e1d8",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_beb67b7b79714ad4852896e2f10124bf"
          }
        },
        "3db5fd5cdbf443c7b8ccf4da4260363c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_042cdf74095c4d08958b608f673fe7a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [04:13&lt;00:00, 25.31s/it, epoch=9, tr_loss=1.96e+3]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e428c7042d1943c2a6c8703273e7e015"
          }
        },
        "83c5f0808cf74dbeb1c54da91cf6e1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "beb67b7b79714ad4852896e2f10124bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "042cdf74095c4d08958b608f673fe7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e428c7042d1943c2a6c8703273e7e015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46f6f8289c7e4e80aba4c65be390ebee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3e69063e32d54462bc52082c7f30d82e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9a0f24225a124aea9b0a79cb0ad7c917",
              "IPY_MODEL_8332e0d4cffd418db7fb7f63dad4e859"
            ]
          }
        },
        "3e69063e32d54462bc52082c7f30d82e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a0f24225a124aea9b0a79cb0ad7c917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9bf4be8ddac49a79c5ac46f540d331e",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1947,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1947,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43638c9633024056a66b8aa6b2ea0f7d"
          }
        },
        "8332e0d4cffd418db7fb7f63dad4e859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59dcad446782403e8af92f535578cae3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.55k/? [00:00&lt;00:00, 8.95kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab1213a973bd425c8641a6851e5f8aea"
          }
        },
        "f9bf4be8ddac49a79c5ac46f540d331e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43638c9633024056a66b8aa6b2ea0f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59dcad446782403e8af92f535578cae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab1213a973bd425c8641a6851e5f8aea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9da64236dba840d5b0dc8cb48eae169f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8773584b9c9f4c70a9aea21b0b2a7e23",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8e5cb9d1efdc47868a0a477ce3ffb0ca",
              "IPY_MODEL_0ee7dc81de144364bf7ac551ce717684"
            ]
          }
        },
        "8773584b9c9f4c70a9aea21b0b2a7e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e5cb9d1efdc47868a0a477ce3ffb0ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2788d29cf5f04e488257d1815db7edc9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 16,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 16,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b698c0c70c8481aabd1f8f5a6cce2a8"
          }
        },
        "0ee7dc81de144364bf7ac551ce717684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb23488ba86a44c6b97a1dceb5e86505",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 16/16 [00:21&lt;00:00,  1.31s/it, val_loss=2.05e+3]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_39a9c86f8a3f438781b4e480eafa820b"
          }
        },
        "2788d29cf5f04e488257d1815db7edc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b698c0c70c8481aabd1f8f5a6cce2a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb23488ba86a44c6b97a1dceb5e86505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "39a9c86f8a3f438781b4e480eafa820b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/export-v3/notebooks/wav2vec2_saved_model_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndG8MjmJeicp"
      },
      "source": [
        "# Fine-tuning with an extra head\n",
        "\n",
        "In this notebook, we will load the pre-trained wav2vec2 model from [TFHub](https://tfhub.dev) and will fine-tune it on [LibriSpeech dataset](https://huggingface.co/datasets/librispeech_asr) by appending LM head over the top of our pre-trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWk8nL6Ui-_0"
      },
      "source": [
        "## Setting Up\n",
        "\n",
        "Before diving into it, let's see what GPU we got using `nvidia-smi`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2DQV2hde_Vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499889e8-3e44-4763-f3c0-88e8a91027ff"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul 23 10:21:04 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hBGUWT_mKkw"
      },
      "source": [
        "The following cell will clone my code repository ([`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2)) and will install all the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seqTlMyeZvM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8d47526-9713-42ef-a27c-3e61bd4213db"
      },
      "source": [
        "!git clone https://github.com/vasudevgupta7/gsoc-wav2vec2 --branch=export-v2\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.chdir(\"gsoc-wav2vec2\")\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "!pip3 install -qe ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gsoc-wav2vec2'...\n",
            "remote: Enumerating objects: 412, done.\u001b[K\n",
            "remote: Counting objects: 100% (412/412), done.\u001b[K\n",
            "remote: Compressing objects: 100% (265/265), done.\u001b[K\n",
            "remote: Total 412 (delta 225), reused 309 (delta 137), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (412/412), 2.90 MiB | 11.24 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 177 kB 39.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 45.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 38.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25h  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuJL8-f0zn5"
      },
      "source": [
        "## Model setup using `TFHub`\n",
        "\n",
        "We will start by importing all the important libraries & modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3_fgx4eZvM7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from wav2vec2 import Wav2Vec2Config\n",
        "\n",
        "config = Wav2Vec2Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuVyJshArx9Q"
      },
      "source": [
        "We will be loading the pre-trained saved-model directly from TFHub. [`hub.load(...)`](https://www.tensorflow.org/hub/api_docs/python/hub/load) will download the pre-trained model first and will call [`tf.saved_model.load(...)`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load) over those downloaded weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0rVUxyWsS5f"
      },
      "source": [
        "First, we will download our model from TFHub & will wrap our model signature with [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to be able to use this model like any other keras layer. Fortunately, `hub.KerasLayer` can do both for us in just 1 line.\n",
        "\n",
        "You can refer this [script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/export2hub.py) in case you are interested in model exporting script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO6QRC7KZvM9"
      },
      "source": [
        "pretrained_layer = hub.KerasLayer(\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\", trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7IwrLqbbkw1"
      },
      "source": [
        "Object `pretrained_layer` is the freezed version of [`Wav2Vec2Model`](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/wav2vec2/modeling.py). Pre-trained weights are converted from HuggingFace PyTorch [pre-trained weights](https://huggingface.co/facebook/wav2vec2-base) using [this script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/convert_torch_to_tf.py).\n",
        "\n",
        "Originally, wav2vec2 was pre-trained with a masked language modelling approach with the objective to identify the true quantized latent speech representation for a masked time step. You can read more about the training objective in the paper- [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SseDnCr7hyhC"
      },
      "source": [
        "Now, we will be defining a few constants and hyper-parameters which will be useful in the next few cells. `AUDIO_MAXLEN` is intentionally set to `246000` as the model signature only accepts static sequence length of `246000`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiILuMBERxlO"
      },
      "source": [
        "AUDIO_MAXLEN = 246000\n",
        "LABEL_MAXLEN = 256\n",
        "BATCH_SIZE = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V4gTgGLgXvO"
      },
      "source": [
        "In the following cell, we will wrap `pretrained_layer` & a dense layer (LM head) with the [TensorFlow's Functional API](https://www.tensorflow.org/guide/keras/functional)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3CUN1KEB10Q"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(AUDIO_MAXLEN,))\n",
        "hidden_states = pretrained_layer(inputs)\n",
        "outputs = tf.keras.layers.Dense(config.vocab_size)(hidden_states)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TD_g9eO4vqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6d8a6d-228b-4ad4-8220-801ddaa9ba55"
      },
      "source": [
        "pretrained_layer(inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 768, 768) dtype=float32 (created by layer 'keras_layer')>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zDXuoMXhDMo"
      },
      "source": [
        "The dense layer (defined above) is having an output dimension of `vocab_size` as we want to predict probabilities of each token in the vocabulary at each time step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPp18ZHRtnq-"
      },
      "source": [
        "## Setting up training state\n",
        "\n",
        "Alright, let's define our training forward pass by calling the model with `training=True` and wrapping it with `tf.function(...)`. It's important to wrap it with `tf.function(...)` to be able to get performance benefits during training.\n",
        "\n",
        "Additionally, we will be passing `jit_compile=True` to compile (using XLA) our model graph on the accelerators (i.e GPUs/TPUs) & fuse many operations to get out-of-box performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f5M8YXXZvM_"
      },
      "source": [
        "@tf.function(jit_compile=True)\n",
        "def forward(batch):\n",
        "    return model(batch, training=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATQy1ZK3vFr7"
      },
      "source": [
        "In TensorFlow, model weights are build only when `model.__call__` is called for the first time, so the following cell will build the model weights for us. Further, we will be running `model.summary()` for checking the total number of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgL5wyaXZvM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a359a7-f6ec-4cf5-f490-abc5a8541f02"
      },
      "source": [
        "forward(tf.random.uniform(shape=(BATCH_SIZE, AUDIO_MAXLEN)))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 246000)]          0         \n",
            "_________________________________________________________________\n",
            "keras_layer (KerasLayer)     (None, 768, 768)          94371712  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 768, 32)           24608     \n",
            "=================================================================\n",
            "Total params: 94,396,320\n",
            "Trainable params: 24,608\n",
            "Non-trainable params: 94,371,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxxA4Fevp7m"
      },
      "source": [
        "Now, we need to define `loss_fn` and optimizer to be able to train the model. The following cell will do that for us. We will be using the `Adam` optimizer for simplicity. `CTCLoss` is a very common loss type that is used for tasks (like `ASR`) where input sub-parts can't be easily aligned with output sub-parts. You can read more about CTC-loss from this amazing [blog post](https://distill.pub/2017/ctc/).\n",
        "\n",
        "\n",
        "`CTCLoss` (from [`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2) package) accepts 3 arguments: `config`, `model_input_shape` & `division_factor`. If `division_factor=1`, then loss will simply get summed, so pass `division_factor` accordingly to get mean over batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glDepVEHZvM_"
      },
      "source": [
        "from wav2vec2 import CTCLoss\n",
        "\n",
        "LEARNING_RATE = 1e-5\n",
        "\n",
        "loss_fn = CTCLoss(config, (BATCH_SIZE, AUDIO_MAXLEN), division_factor=BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mvTuOXpwsQe"
      },
      "source": [
        "## Loading & Pre-processing data\n",
        "\n",
        "Let's now download the LibriSpeech dataset from the [official website](http://www.openslr.org/12) and set it up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4kIEC77cBCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3812fe-04e4-4da5-e576-3335aaf05201"
      },
      "source": [
        "!wget https://www.openslr.org/resources/12/dev-clean.tar.gz -P ./data/train/\n",
        "!tar -xf ./data/train/dev-clean.tar.gz -C ./data/train/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-23 10:23:25--  https://www.openslr.org/resources/12/dev-clean.tar.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 337926286 (322M) [application/x-gzip]\n",
            "Saving to: ‘./data/train/dev-clean.tar.gz’\n",
            "\n",
            "dev-clean.tar.gz    100%[===================>] 322.27M  20.0MB/s    in 17s     \n",
            "\n",
            "2021-07-23 10:23:43 (18.5 MB/s) - ‘./data/train/dev-clean.tar.gz’ saved [337926286/337926286]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsQpmpn6jrMI"
      },
      "source": [
        "**Note:** We are using `dev-clean` configuration as this notebook is just for demonstration purposes, so we just need small data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynxAjtGHGFpM",
        "outputId": "42fe9298-a723-40ae-c8df-7611cbd7ee8f"
      },
      "source": [
        "ls ./data/train/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev-clean.tar.gz  \u001b[0m\u001b[01;34mLibriSpeech\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBMiORo0xJD0"
      },
      "source": [
        "Our dataset lies in `LibriSpeech` directory. Let's further narrow down & choose a sub-directory to see few files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkIu_Wt4ZvNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46814a99-876d-4f9a-e8ff-cd8e3e64d479"
      },
      "source": [
        "data_dir = \"./data/train/LibriSpeech/dev-clean/2428/83705/\"\n",
        "all_files = os.listdir(data_dir)\n",
        "\n",
        "flac_files = [f for f in all_files if f.endswith(\".flac\")]\n",
        "txt_files = [f for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "print(\"Transcription files:\", txt_files, \"\\nSound files:\", flac_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transcription files: ['2428-83705.trans.txt'] \n",
            "Sound files: ['2428-83705-0016.flac', '2428-83705-0005.flac', '2428-83705-0007.flac', '2428-83705-0017.flac', '2428-83705-0020.flac', '2428-83705-0011.flac', '2428-83705-0037.flac', '2428-83705-0021.flac', '2428-83705-0034.flac', '2428-83705-0019.flac', '2428-83705-0030.flac', '2428-83705-0018.flac', '2428-83705-0033.flac', '2428-83705-0027.flac', '2428-83705-0040.flac', '2428-83705-0032.flac', '2428-83705-0008.flac', '2428-83705-0010.flac', '2428-83705-0006.flac', '2428-83705-0001.flac', '2428-83705-0029.flac', '2428-83705-0024.flac', '2428-83705-0036.flac', '2428-83705-0038.flac', '2428-83705-0025.flac', '2428-83705-0009.flac', '2428-83705-0035.flac', '2428-83705-0023.flac', '2428-83705-0028.flac', '2428-83705-0004.flac', '2428-83705-0014.flac', '2428-83705-0031.flac', '2428-83705-0000.flac', '2428-83705-0022.flac', '2428-83705-0026.flac', '2428-83705-0042.flac', '2428-83705-0002.flac', '2428-83705-0041.flac', '2428-83705-0013.flac', '2428-83705-0012.flac', '2428-83705-0043.flac', '2428-83705-0039.flac', '2428-83705-0015.flac', '2428-83705-0003.flac']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEObi_Apk3ZD"
      },
      "source": [
        "Alright, so each sub-directory is having many `.flac` files and single `.txt` file. `.txt` file will have text transcriptions for all the speech samples (i.e. `.flac` files) present in that sub-directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYW6WKJflO2e"
      },
      "source": [
        "In following cell, we will define function for loading & formatting the text data into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEBKxQblHPwq"
      },
      "source": [
        "def read_txt_file(f):\n",
        "  with open(f, \"r\") as f:\n",
        "    samples = f.read().split(\"\\n\")\n",
        "    samples = {s.split()[0]: \" \".join(s.split()[1:]) for s in samples if len(s.split()) > 2}\n",
        "  return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldkf_ceb0_YW"
      },
      "source": [
        "Similary, we will define a function for loading speech sample from `.flac` file.\n",
        "\n",
        "`REQUIRED_SAMPLE_RATE` is set to `16000` as wav2vec2 was pre-trained with `16K` frequency and it's recommended to train it further without any major change in data distribution due to frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJ3OzPsTyXv"
      },
      "source": [
        "import soundfile as sf\n",
        "\n",
        "REQUIRED_SAMPLE_RATE = 16000\n",
        "\n",
        "def read_flac_file(file_path):\n",
        "  with open(file_path, \"rb\") as f:\n",
        "      audio, sample_rate = sf.read(f)\n",
        "  if sample_rate != REQUIRED_SAMPLE_RATE:\n",
        "      raise ValueError(\n",
        "          f\"sample rate (={sample_rate}) of your files must be {REQUIRED_SAMPLE_RATE}\"\n",
        "      )\n",
        "  file_id = os.path.split(file_path)[-1][:-len(\".flac\")]\n",
        "  return {file_id: audio}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI5J-2Dfm_wT"
      },
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "file_id = \"2428-83705-0016\"\n",
        "\n",
        "file_path = os.path.join(data_dir, f'{file_id}.flac')\n",
        "print(read_txt_file(file_path)[file_id])\n",
        "Audio(filename=file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8jJ7Ed81p_A"
      },
      "source": [
        "Now, we will combine all the speech & text samples and will define the function (in next cell) for that purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI-5YCzaTsei"
      },
      "source": [
        "def fetch_sound_text_mapping(data_dir):\n",
        "  all_files = os.listdir(data_dir)\n",
        "\n",
        "  flac_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".flac\")]\n",
        "  txt_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "  txt_samples = {}\n",
        "  for f in txt_files:\n",
        "    txt_samples.update(read_txt_file(f))\n",
        "\n",
        "  speech_samples = {}\n",
        "  for f in flac_files:\n",
        "    speech_samples.update(read_flac_file(f))\n",
        "\n",
        "  assert len(txt_samples) == len(speech_samples)\n",
        "\n",
        "  samples = [(txt_samples[file_id], speech_samples[file_id]) for file_id in speech_samples.keys()]\n",
        "  return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx95Lxvu0nT4"
      },
      "source": [
        "It's time to have a look at a few samples ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ls7X_jqIz4R",
        "outputId": "aa4b2fb3-c762-4301-eb50-0ffe7e1f72af"
      },
      "source": [
        "samples = fetch_sound_text_mapping(data_dir)\n",
        "samples[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('THERE WERE NO SIGNS OF FALTERING ABOUT HER FLOW OF LANGUAGE',\n",
              "  array([-0.00036621, -0.00015259, -0.00012207, ..., -0.0005188 ,\n",
              "         -0.00048828, -0.00048828])),\n",
              " ('FOR INSTANCE LOOK AT THEIR BEHAVIOUR IN THE MATTER OF THE RING',\n",
              "  array([-0.00201416, -0.0022583 , -0.00234985, ...,  0.00137329,\n",
              "          0.0012207 ,  0.00109863])),\n",
              " (\"THE GIRL IS FRETTING BUT YOU DON'T SEEM TO NOTICE IT\",\n",
              "  array([6.71386719e-04, 6.71386719e-04, 5.49316406e-04, ...,\n",
              "         2.44140625e-04, 2.44140625e-04, 3.05175781e-05])),\n",
              " ('I FOUND THAT AS A WOMAN OF BUSINESS SHE WAS BEYOND ALL MY EXPECTATIONS',\n",
              "  array([-2.74658203e-04, -6.10351562e-04, -4.57763672e-04, ...,\n",
              "          0.00000000e+00, -3.05175781e-05,  0.00000000e+00])),\n",
              " ('IT WAS PLAIN THAT TOGETHER WE SHOULD MANAGE MOST COMFORTABLY DELIGHTFULLY IN FACT',\n",
              "  array([-9.15527344e-05,  9.15527344e-05, -1.83105469e-04, ...,\n",
              "         -5.79833984e-04, -4.88281250e-04, -3.96728516e-04]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg8Zia1kzw0J"
      },
      "source": [
        "Let's pre-process the data now !!!\n",
        "\n",
        "We will first define the tokenizer & processor using gsoc-wav2vec2 package.Then, we will do very simple pre-processing. Speech will be normalized over time axis and text will be tokenized using `processor` and `tokenizer` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaat_hMLNVHF",
        "outputId": "3d0901b4-8985-48ff-a464-9c7ff95f5e44"
      },
      "source": [
        "from wav2vec2 import Wav2Vec2Processor\n",
        "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
        "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  label = tokenizer(text)\n",
        "  return tf.constant(label, dtype=tf.int32)\n",
        "\n",
        "def preprocess_speech(audio):\n",
        "  audio = tf.constant(audio, dtype=tf.float32)\n",
        "  return processor(tf.transpose(audio))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading `vocab.json` from https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/vocab.json ... DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyKl8QP-zRFC"
      },
      "source": [
        "Now, we will define the python generator to call the preprocessing functions we defined in above cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoQrRalwMpQ6"
      },
      "source": [
        "def inputs_generator(samples):\n",
        "  for text, speech in samples:\n",
        "    yield preprocess_text(text), preprocess_speech(speech)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GruDUhROZwD",
        "outputId": "66d641a2-bd14-456b-d250-d044062353fe"
      },
      "source": [
        "from functools import partial\n",
        "generator = partial(inputs_generator, samples=samples)\n",
        "next(iter(generator()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(59,), dtype=int32, numpy=\n",
              " array([ 6, 11,  5, 13,  5,  4, 18,  5, 13,  5,  4,  9,  8,  4, 12, 10, 21,\n",
              "         9, 12,  4,  8, 20,  4, 20,  7, 15,  6,  5, 13, 10,  9, 21,  4,  7,\n",
              "        24,  8, 16,  6,  4, 11,  5, 13,  4, 20, 15,  8, 18,  4,  8, 20,  4,\n",
              "        15,  7,  9, 21, 16,  7, 21,  5], dtype=int32)>,\n",
              " <tf.Tensor: shape=(58240,), dtype=float32, numpy=\n",
              " array([-0.00590616, -0.00194895, -0.00138363, ..., -0.00873275,\n",
              "        -0.00816743, -0.00816743], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vlm3ySFULsG"
      },
      "source": [
        "## Setting up `tf.data.Dataset`\n",
        "\n",
        "Following cell will setup `tf.data.Dataset` object using its `.from_generator(...)` method. We will be using the `generator` object, we defined in the above cell.\n",
        "\n",
        "**Note:** For distributed training (especially on TPUs), `.from_generator(...)` doesn't work currently & it is recommended to train on data stored in `.tfrecord` format. You can refer to [this script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/make_tfrecords.py) for more details on how to convert LibriSpeech data into tfrecords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbQ_dMwGO62h"
      },
      "source": [
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None),  dtype=tf.float32),\n",
        ")\n",
        "dataset = tf.data.Dataset.from_generator(generator, output_signature=output_signature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1eDBZjkogsG"
      },
      "source": [
        "Let's shuffle the dataset using `.shuffle(...)` method. Argument buffer size leads to approximate shuffling as many times the complete dataset can't be fitted into memory for actual shuffling (Eg. complete LibriSpeech tfrecords takes around 250 GB on disk)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXBbNsRyPyw3"
      },
      "source": [
        "BUFFER_SIZE = len(flac_files)\n",
        "SEED = 42\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE, seed=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DAUmns3pXfr"
      },
      "source": [
        "We will pass the dataset into multiple batches, so let's prepare batches in the following cell. Now, all the sequences in a batch should be padded to a constant length. We will use the`.padded_batch(...)` method for that purpose. We also need to restrict sequence length to some particular value as some of the sequences are very long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okhko1IWRida"
      },
      "source": [
        "dataset = dataset.map(lambda labels, speech: (labels[: LABEL_MAXLEN], speech[: AUDIO_MAXLEN]))\n",
        "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(LABEL_MAXLEN, AUDIO_MAXLEN), padding_values=(0, 0.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A45CjQG5qSbV"
      },
      "source": [
        "Accelerators (like GPUs/TPUs) are very fast and often data-loading (& pre-processing) becomes the bottleneck during training as the data-loading part happens on CPUs. This can increase the training time significantly especially when there is a lot of online pre-processing involved or data is streamed online from GCS buckets. To handle those issues, `tf.data.Dataset` offers the `.prefetch(...)` method. This method helps in preparing the next few batches in parallel (on CPUs) while the model is making predictions (on GPUs/TPUs) on the current batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-bKu2YjRior"
      },
      "source": [
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqk2cs6LxVIh"
      },
      "source": [
        "Since this notebook is made for demonstration purposes, we will be taking first `num_batches` and will perform training over only that. You are encouraged to train on the whole dataset though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6GO5oYUxXtz"
      },
      "source": [
        "num_batches = 16\n",
        "dataset = dataset.take(num_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJtSLgCvxvBG"
      },
      "source": [
        "## Training\n",
        "\n",
        "Let's define our `train_step` function now. There are 3 main steps in `train_step`: \n",
        "1. forward pass with variables tracking\n",
        "2. backward pass for calculating gradients\n",
        "3. variables update to minimize training loss\n",
        "\n",
        "All the trainable variables in the scope of `tf.GradientTape(...)` will get tracked during the forward pass. Further, `.gradient(...)` will help us find gradient of loss w.r.to those tracked variables & `.apply_gradients(...)` will update the trainable variables based on our `optimizer` defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RYI8ra_ZvNA"
      },
      "source": [
        "@tf.function\n",
        "def train_step(speech, labels):\n",
        "    with tf.GradientTape() as gtape:\n",
        "        speech = forward(speech)\n",
        "        loss = loss_fn(labels, speech)\n",
        "    grads = gtape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOuhcSPkytaN"
      },
      "source": [
        "Let's kick start training finally !!!\n",
        "\n",
        "We will iterate over our dataset (instance of `tf.data.Dataset`) and each batch will be fed to `train_step(...)` for calculating loss, gradients & updating parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUujvZn4ZvNA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "9ab42871d4ce4ec3926439d2db14c766",
            "672481d2d0164171bfb0fd0d08916bd7",
            "73ea04eded764748ad24c6c6e63a84e6",
            "3db5fd5cdbf443c7b8ccf4da4260363c",
            "83c5f0808cf74dbeb1c54da91cf6e1d8",
            "beb67b7b79714ad4852896e2f10124bf",
            "042cdf74095c4d08958b608f673fe7a0",
            "e428c7042d1943c2a6c8703273e7e015"
          ]
        },
        "outputId": "2c683b43-8d30-4d1e-b8ab-abfd566bcd40"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "EPOCHS = 10\n",
        "\n",
        "pbar = tqdm(range(EPOCHS), total=EPOCHS)\n",
        "for e in pbar:\n",
        "  running_loss, steps = tf.constant(0.), 0\n",
        "  for labels, speech in dataset:\n",
        "      loss = train_step(speech, labels)\n",
        "      running_loss += loss\n",
        "      steps += 1\n",
        "  pbar.set_postfix(tr_loss=running_loss.numpy().item()/steps, epoch=e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ab42871d4ce4ec3926439d2db14c766",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1408: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1391: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJfPlTgezD0i"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Let's compute loss over validation dataset using `eval_step(...)` defined in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssWXWc7CZvNB"
      },
      "source": [
        "@tf.function(jit_compile=True)\n",
        "def eval_fwd(batch):\n",
        "  return model(batch, training=False)\n",
        "\n",
        "@tf.function\n",
        "def eval_step(speech, labels):\n",
        "    speech = eval_fwd(speech)\n",
        "    loss = loss_fn(labels, speech)\n",
        "    return loss, tf.argmax(speech, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_91Y7-r3xu"
      },
      "source": [
        "We need to compute `WER` (word error rate) over our validation data. We will use `load_metric(...)` function from [HuggingFace datasets](https://huggingface.co/docs/datasets/) library. Let's first install the `datasets` library using `pip` and then define the `metric` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "46f6f8289c7e4e80aba4c65be390ebee",
            "3e69063e32d54462bc52082c7f30d82e",
            "9a0f24225a124aea9b0a79cb0ad7c917",
            "8332e0d4cffd418db7fb7f63dad4e859",
            "f9bf4be8ddac49a79c5ac46f540d331e",
            "43638c9633024056a66b8aa6b2ea0f7d",
            "59dcad446782403e8af92f535578cae3",
            "ab1213a973bd425c8641a6851e5f8aea"
          ]
        },
        "id": "GW9F_oVDU1TZ",
        "outputId": "6a6176be-647a-4bc4-9b6b-e9dd87d180de"
      },
      "source": [
        "!pip3 install -q datasets\n",
        "\n",
        "from datasets import load_metric\n",
        "metric = load_metric(\"wer\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 542 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 65.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 118 kB 81.2 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46f6f8289c7e4e80aba4c65be390ebee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1947.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFh1myg1x4ua"
      },
      "source": [
        "It's time to run the evaluation on validation data now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQTFVjZghckJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9da64236dba840d5b0dc8cb48eae169f",
            "8773584b9c9f4c70a9aea21b0b2a7e23",
            "8e5cb9d1efdc47868a0a477ce3ffb0ca",
            "0ee7dc81de144364bf7ac551ce717684",
            "2788d29cf5f04e488257d1815db7edc9",
            "0b698c0c70c8481aabd1f8f5a6cce2a8",
            "fb23488ba86a44c6b97a1dceb5e86505",
            "39a9c86f8a3f438781b4e480eafa820b"
          ]
        },
        "outputId": "a022c310-4425-41bb-a8d5-2f4e3bac8a59"
      },
      "source": [
        "pbar = tqdm(dataset, total=num_batches)\n",
        "for labels, speech in pbar:\n",
        "    loss, predictions = eval_step(speech, labels)\n",
        "    pbar.set_postfix(val_loss=loss.numpy().item())\n",
        "    predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "    references = [tokenizer.decode(label, group_tokens=False) for label in labels.numpy().tolist()]\n",
        "    metric.add_batch(references=references, predictions=predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9da64236dba840d5b0dc8cb48eae169f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCc8qBesv3e"
      },
      "source": [
        "We are using the `tokenizer.decode(...)` method for decoding our predictions and labels back into the text and will add them to the metric for `WER` computation later.\n",
        "\n",
        "**Note:** We are using the same dataset just for demonstration purposes. In general, we should use separate data (generally called `validation/dev` data) sampled before initiating training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI_URj8Wtb2g"
      },
      "source": [
        "`metirc.compute()` will calculate the final WER score over all the batches added in previous cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a83wekLgWMod",
        "outputId": "966a97e6-c5f8-4151-841d-845106790ccc"
      },
      "source": [
        "metric.compute()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0039138943248533"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_cD1OgVEjl4"
      },
      "source": [
        "**Note:** Here metric value doesn't make any sense as the model is trained on very small data and ASR-like tasks often require a very large amount of data to learn a mapping from speech to text. You should probably train on large data to get some good results. This notebook is just for showing the workflow of training a saved model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzAOI78tky08"
      },
      "source": [
        "### Wrapping training in `tf.keras.Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBEp6soJkd7Y"
      },
      "source": [
        "class Trainer(tf.keras.Model):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "\n",
        "  def compile(self, optimizer, loss_fn):\n",
        "      super().compile(optimizer=optimizer)\n",
        "      self.loss_fn = loss_fn\n",
        "\n",
        "  @tf.function(jit_compile=True)\n",
        "  def call(self, speech, training=False):\n",
        "      return self.model(speech, training=training)\n",
        "\n",
        "  def train_step(speech, labels):\n",
        "      with tf.GradientTape() as gtape:\n",
        "          speech = self(speech, training=True)\n",
        "          loss = self.loss_fn(labels, speech)\n",
        "      grads = gtape.gradient(loss, self.model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "      return loss\n",
        "\n",
        "  @tf.function\n",
        "  def eval_step(speech, labels):\n",
        "      speech = self(speech, training=False)\n",
        "      loss = self.loss_fn(labels, speech)\n",
        "      return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuBY2sZElgwg"
      },
      "source": [
        "trainer = Trainer(model)\n",
        "trainer.compile(optimizer, loss_fn=loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtuSfnj1l-I_"
      },
      "source": [
        "trainer.fit(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXC757bztJc"
      },
      "source": [
        "Finally, we have reached an end to this notebook. But it's not an end of learning TensorFlow for speech-related tasks, this [repository](https://github.com/vasudevgupta7/gsoc-wav2vec2) contains some more amazing tutorials. Feel free to go through them. You can also refer to [this repositary](https://github.com/tulasiram58827/TTS_TFLite) for some more amazing tutorials on speech-related tasks. In case you encountered any bug in this notebook, please create an issue [here](https://github.com/vasudevgupta7/gsoc-wav2vec2/issues)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBEm6caxYDyK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}